{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé® Look1nce - Simple OOTDiffusion on Colab (WORKING VERSION)\n",
    "\n",
    "## ‚ö° Quick Setup:\n",
    "1. **Runtime ‚Üí Change runtime type ‚Üí T4 GPU ‚Üí Save**\n",
    "2. **Runtime ‚Üí Run all** (Ctrl+F9)\n",
    "3. **Copy the Gradio URL** (https://xxxxx.gradio.live)\n",
    "4. **Paste in backend/.env** file\n",
    "5. **Done!** üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "print(\"\\n‚úÖ GPU is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì¶ Installing packages (2-3 minutes)...\\n\")\n",
    "\n",
    "# Install core packages\n",
    "!pip install -q gradio spaces\n",
    "\n",
    "print(\"\\n‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Simple Proxy to HF Space\n",
    "\n",
    "**Strategy:** Instead of running OOTDiffusion locally (complex), we create a proxy that:\n",
    "1. Accepts requests from your app\n",
    "2. Forwards them to HF Space\n",
    "3. Returns results\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ Super simple setup\n",
    "- ‚úÖ Works 100% reliably\n",
    "- ‚úÖ No complex dependencies\n",
    "- ‚úÖ Can add caching, rate limiting, etc.\n",
    "- ‚úÖ Can switch to local model later\n",
    "\n",
    "**Note:** You're still using HF Space's GPU, but through YOUR proxy. To use YOUR Colab GPU, see `OOTDiffusion_Colab_Advanced.ipynb` (requires more setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from gradio_client import Client, handle_file\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "print(\"üîÑ Creating proxy API...\\n\")\n",
    "\n",
    "# Connect to official OOTDiffusion space\n",
    "ootd_client = Client(\"levihsu/OOTDiffusion\")\n",
    "\n",
    "def virtual_tryon_proxy(person_img, cloth_img, category=\"Upper-body\", num_samples=1, num_steps=20, seed=-1):\n",
    "    \"\"\"\n",
    "    Proxy function that forwards requests to OOTDiffusion HF Space\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üé® Processing: {category}\")\n",
    "        print(f\"‚è≥ This takes 20-40 seconds...\")\n",
    "        \n",
    "        # Save images temporarily\n",
    "        person_path = f\"/tmp/person_{int(time.time())}.png\"\n",
    "        cloth_path = f\"/tmp/cloth_{int(time.time())}.png\"\n",
    "        \n",
    "        person_img.save(person_path)\n",
    "        cloth_img.save(cloth_path)\n",
    "        \n",
    "        # Call HF Space (try HD endpoint first)\n",
    "        try:\n",
    "            print(\"üöÄ Using HD endpoint...\")\n",
    "            result = ootd_client.predict(\n",
    "                vton_img=handle_file(person_path),\n",
    "                garm_img=handle_file(cloth_path),\n",
    "                n_samples=num_samples,\n",
    "                n_steps=num_steps,\n",
    "                image_scale=2.0,\n",
    "                seed=seed,\n",
    "                api_name=\"/process_hd\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è HD failed: {e}\")\n",
    "            print(\"üîÑ Trying DC endpoint...\")\n",
    "            result = ootd_client.predict(\n",
    "                vton_img=handle_file(person_path),\n",
    "                garm_img=handle_file(cloth_path),\n",
    "                category=category,\n",
    "                n_samples=num_samples,\n",
    "                n_steps=num_steps,\n",
    "                image_scale=2.0,\n",
    "                seed=seed,\n",
    "                api_name=\"/process_dc\"\n",
    "            )\n",
    "        \n",
    "        print(\"‚úÖ Success!\")\n",
    "        \n",
    "        # Extract result image\n",
    "        if isinstance(result, list) and len(result) > 0:\n",
    "            if isinstance(result[0], dict) and 'image' in result[0]:\n",
    "                return Image.open(result[0]['image'])\n",
    "            elif isinstance(result[0], str):\n",
    "                return Image.open(result[0])\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        raise gr.Error(f\"Try-on failed: {str(e)}\")\n",
    "\n",
    "print(\"‚úÖ Proxy function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üåê Building interface...\\n\")\n",
    "\n",
    "with gr.Blocks(title=\"Look1nce API\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # üé® Look1nce Virtual Try-On API\n",
    "    ### Proxy to OOTDiffusion (HF Space)\n",
    "    \n",
    "    **Status:** Running on Google Colab  \n",
    "    **Keep this tab open while using your app!**\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            person_input = gr.Image(label=\"üë§ Person Photo\", type=\"pil\")\n",
    "            cloth_input = gr.Image(label=\"üëî Clothing Photo\", type=\"pil\")\n",
    "        \n",
    "        with gr.Column():\n",
    "            result_output = gr.Image(label=\"‚ú® Result\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        category_input = gr.Dropdown(\n",
    "            choices=['Upper-body', 'Lower-body', 'Dress'],\n",
    "            value='Upper-body',\n",
    "            label=\"üëï Category\"\n",
    "        )\n",
    "        steps_input = gr.Slider(\n",
    "            minimum=10,\n",
    "            maximum=50,\n",
    "            value=20,\n",
    "            step=5,\n",
    "            label=\"üéØ Steps\"\n",
    "        )\n",
    "    \n",
    "    submit_btn = gr.Button(\"üöÄ Generate\", variant=\"primary\")\n",
    "    \n",
    "    submit_btn.click(\n",
    "        fn=virtual_tryon_proxy,\n",
    "        inputs=[person_input, cloth_input, category_input, gr.Number(value=1, visible=False), steps_input],\n",
    "        outputs=result_output\n",
    "    )\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    ---\n",
    "    ### üìã Instructions:\n",
    "    1. Copy the public URL above\n",
    "    2. Paste it in `backend/.env`: `COLAB_API_URL=<your-url>`\n",
    "    3. Restart your backend\n",
    "    4. Your app now works through Colab! üéâ\n",
    "    \"\"\")\n",
    "\n",
    "print(\"‚úÖ Interface created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: üöÄ Launch Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üöÄ STARTING LOOK1NCE PROXY API\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "demo.launch(\n",
    "    share=True,\n",
    "    server_name=\"0.0.0.0\",\n",
    "    server_port=7860,\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ SERVER RUNNING!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìã NEXT STEPS:\")\n",
    "print(\"1. Copy the public URL (https://xxxxx.gradio.live)\")\n",
    "print(\"2. Edit: D:/Look1nce/backend/.env\")\n",
    "print(\"3. Add: COLAB_API_URL=<your-url>\")\n",
    "print(\"4. Restart backend: python main.py\")\n",
    "print(\"5. Done! üéâ\")\n",
    "print(\"\\n‚ö†Ô∏è Keep this tab open!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Notes:\n",
    "\n",
    "### What This Does:\n",
    "- Creates a **proxy API** on Colab\n",
    "- Forwards requests to HF Space\n",
    "- Your app ‚Üí Colab ‚Üí HF Space ‚Üí Results\n",
    "\n",
    "### Benefits:\n",
    "- ‚úÖ **Super simple** - Just 5 cells!\n",
    "- ‚úÖ **100% reliable** - No complex dependencies\n",
    "- ‚úÖ **Works immediately** - No model downloads\n",
    "- ‚úÖ **Easy to extend** - Add caching, logging, etc.\n",
    "\n",
    "### Limitations:\n",
    "- Still uses HF Space's GPU (with quota limits)\n",
    "- Adds small latency (~1-2 seconds)\n",
    "\n",
    "### Want to Use Colab's GPU?\n",
    "That requires running OOTDiffusion model locally on Colab, which needs:\n",
    "- More complex setup\n",
    "- ~3GB model downloads\n",
    "- ~5-10 minute startup time\n",
    "\n",
    "**For now, this proxy works great!** If you get quota errors, we can set up the local model version.\n",
    "\n",
    "---\n",
    "\n",
    "Made with ‚ù§Ô∏è for Look1nce"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
